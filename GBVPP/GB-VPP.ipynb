{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c ventilator-pressure-prediction\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "DIR = ''\n",
    "file = 'ventilator-pressure-prediction.zip'\n",
    "with zipfile.ZipFile(os.path.join(DIR, file), 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "pd.set_option('display.max_columns', 100)\n",
    "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prepare_csv(csv_name = 'train.csv', lo=-2, hi=8):\n",
    "  df = pd.read_csv(csv_name)\n",
    "  df['cross']= df['u_in'] * df['u_out']\n",
    "  df['cross2']= df['time_step'] * df['u_out']\n",
    "  df['area'] = df['time_step'] * df['u_in']\n",
    "  df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
    "  df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n",
    "  df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
    "  df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
    "  df['one'] = 1\n",
    "  df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
    "  df['u_in_cummean'] = df['u_in_cumsum'] / df['count']\n",
    "  df = df.drop(columns=['one', 'count'])\n",
    "  for shift in range(1, hi + 1):\n",
    "    df['u_in_lag_{}'.format(shift)] = df.groupby(df['breath_id'])['u_in'].shift(shift).fillna(0)\n",
    "    df['u_in_dff_{}'.format(shift)] = df['u_in'] - df['u_in_lag_{}'.format(shift)]\n",
    "  for shift in range(1, hi + 1):\n",
    "    df['u_out_lag_{}'.format(shift)] = df.groupby(df['breath_id'])['u_out'].shift(shift).fillna(0) \n",
    "    df['u_out_dff_{}'.format(shift)] = df['u_out'] - df['u_out_lag_{}'.format(shift)]\n",
    "\n",
    "  df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
    "  df['rolling_sum_4'] = df.groupby('breath_id')['u_in'].rolling(4, min_periods=0).sum().fillna(0).reset_index(level=0, drop=True)\n",
    "  df['rolling_mean_4'] = df.groupby('breath_id')['u_in'].rolling(4, min_periods=0).mean().fillna(0).reset_index(level=0, drop=True)\n",
    "  df['rolling_max_4'] = df.groupby('breath_id')['u_in'].rolling(4, min_periods=0).max().fillna(0).reset_index(level=0, drop=True)\n",
    "  df['rolling_median_4'] = df.groupby('breath_id')['u_in'].rolling(4, min_periods=0).median().fillna(0).reset_index(level=0, drop=True)\n",
    "  df['rolling_sum_8'] = df.groupby('breath_id')['u_in'].rolling(8, min_periods=0).sum().fillna(0).reset_index(level=0, drop=True)\n",
    "  df['rolling_mean_8'] = df.groupby('breath_id')['u_in'].rolling(8, min_periods=0).mean().fillna(0).reset_index(level=0, drop=True)\n",
    "  df['rolling_max_8'] = df.groupby('breath_id')['u_in'].rolling(8, min_periods=0).max().fillna(0).reset_index(level=0, drop=True)\n",
    "  df['rolling_median_8'] = df.groupby('breath_id')['u_in'].rolling(8, min_periods=0).median().fillna(0).reset_index(level=0, drop=True)\n",
    "  df['R'] = df['R'].astype(str)\n",
    "  df['C'] = df['C'].astype(str)\n",
    "  df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
    "  df = pd.get_dummies(df)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('prepared.csv'):\n",
    "  train_df = pd.read_csv('prepared.csv')\n",
    "else:\n",
    "  train_df = read_prepare_csv('train.csv')\n",
    "# print(train_df.corr()['pressure'])\n",
    "test = np.array(train_df['pressure'], dtype=np.float32)\n",
    "train = np.array(train_df.drop(columns=['breath_id', 'id', 'pressure']), dtype=np.float32)\n",
    "\n",
    "gc.collect()\n",
    "train = scaler.fit_transform(train)\n",
    "train = train.reshape(-1, 80, train.shape[1])\n",
    "test = test.reshape(-1, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.layers import Concatenate, LSTM, GRU\n",
    "from tensorflow.keras.layers import Bidirectional, Multiply\n",
    "\n",
    "\n",
    "def build_model(seed, shape=train.shape[-2:]):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    x_input = Input(shape=shape)\n",
    "    \n",
    "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n",
    "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
    "    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n",
    "    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n",
    "    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n",
    "    \n",
    "    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n",
    "    \n",
    "    z31 = Multiply()([x3, z2])\n",
    "    z31 = BatchNormalization()(z31)\n",
    "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n",
    "    \n",
    "    z41 = Multiply()([x4, z3])\n",
    "    z41 = BatchNormalization()(z41)\n",
    "    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n",
    "    \n",
    "    z51 = Multiply()([x5, z4])\n",
    "    z51 = BatchNormalization()(z51)\n",
    "    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n",
    "    \n",
    "    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n",
    "    \n",
    "    x = Dense(units=128, activation='elu')(x)\n",
    "    \n",
    "    x_output = Dense(units=1)(x)\n",
    "\n",
    "    model = keras.Model(inputs=x_input, outputs=x_output, \n",
    "                  name='gb-vpp-model')\n",
    "\n",
    "    optimizer = keras.optimizers.Nadam(.001)\n",
    "    loss = keras.losses.mean_absolute_error\n",
    "    model.compile(optimizer, loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 50:\n",
    "      return lr\n",
    "    else:\n",
    "      return lr * .985\n",
    "\n",
    "callbacks=[]\n",
    "\n",
    "for i in range(2):\n",
    "  callbacks.append(keras.callbacks.ModelCheckpoint(filepath='gdrive/MyDrive/' + 'model_{}.h5'.format(i), save_best_only=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "seeds = [42, 1337, 666666]\n",
    "models = []\n",
    "with tpu_strategy.scope():\n",
    "  for index,seed in enumerate(seeds):\n",
    "    models.append(build_model(seed))\n",
    "    print(len(models), index)\n",
    "    train, test = shuffle(train, test)\n",
    "    gc.collect()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, test, test_size=0.15)\n",
    "    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    lrscheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    models[index].fit(X_train, y_train, batch_size=512, epochs=150, validation_data=(X_test, y_test), callbacks=[lrscheduler, callbacks[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del train_df\n",
    "del  train, test, X_train, X_test, y_train, y_test\n",
    "gc.collect()\n",
    "test_df = read_prepare_csv('test.csv')\n",
    "test_df = test_df.drop(columns=['id', 'breath_id'])\n",
    "test_vals = np.array(test_df, dtype=np.float32)\n",
    "del test_df\n",
    "gc.collect()\n",
    "test_vals = scaler.transform(test_vals)\n",
    "test_vals = test_vals.reshape(-1, 80, test_vals.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "gc.collect()\n",
    "for index, model in enumerate(models):\n",
    "  preds.append(model.predict(test_vals).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_a = np.median(preds, 0)\n",
    "indices = np.arange(1, sub_a.shape[0] + 1)\n",
    "sub_ = pd.DataFrame({'id': indices, 'pressure': sub_a})\n",
    "sub_.to_csv(\"vote_submit_lstm.csv\", index=False)\n",
    "!kaggle competitions submit -c ventilator-pressure-prediction -f vote_submit_lstm.csv -m \"vs\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
