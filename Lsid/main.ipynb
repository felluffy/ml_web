{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.animation as manim\n",
    "import os\n",
    "import rawpy\n",
    "import skimage.measure as sK_measure\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "ds_folder = './Sony/'\n",
    "try: \n",
    "    os.makedirs(ds_folder)\n",
    "except Exception as e:\n",
    "    pass\n",
    "ds_exists = False\n",
    "ds_train_sub = 'sony'\n",
    "# for folder in os.listdir(ds_folder):\n",
    "#     if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_exp_img = 'lei'\n",
    "high_exp_img = 'hei'\n",
    "\n",
    "def pack_raw(raw):\n",
    "    # pack Bayer image to 4 channels\n",
    "    im = raw.raw_image_visible.astype(np.float32)\n",
    "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
    "\n",
    "    im = np.expand_dims(im, axis=2)\n",
    "    img_shape = im.shape\n",
    "    H = img_shape[0]\n",
    "    W = img_shape[1]\n",
    "\n",
    "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
    "                          im[0:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_path, main_dir='Sony/', res=(512,512), tp='bayer', transforms=None):\n",
    "        self.main_dir = main_dir\n",
    "        self.Xs = []\n",
    "        with open(text_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                idx = line.find(' ')\n",
    "                # self.Xs[line[:idx]] = line[idx+1:]\n",
    "                self.Xs.append((line[:idx], line[idx+1:]))\n",
    "        self.transforms = transforms\n",
    "        self.ids = np.arange(len(self.Xs))\n",
    "    def __len__(self):\n",
    "        return len(self.Xs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0: \n",
    "            self.ids = np.random.permutation(self.ids)\n",
    "        sample = {low_exp_img: np.random.rand(1, 1024, 1024,4), high_exp_img: np.random.rand(1, 2048, 2048 ,3)}\n",
    "        if self.transforms != None:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample\n",
    "    \n",
    "        # return torch.rand(1024), torch.rand(1024)\n",
    "        x_img_info, gt_img_info = self.Xs[self.ids[idx]]\n",
    "        x_img_path = os.path.join(self.main_dir, x_img_info)\n",
    "        gt_img_path = gt_img_info.split(' ')[0]\n",
    "        gt_img_path = os.path.join(self.main_dir, gt_img_path)\n",
    "        # print(x_img_path, gt_img_path)\n",
    "        base_x, base_gt = x_img_path.split('/')[-1], gt_img_path.split('/')[-1]\n",
    "        in_exposure = float(base_x[9:-5])\n",
    "        gt_exposure = float(base_gt[9:-5])\n",
    "        ratio = min(gt_exposure / in_exposure, 300)\n",
    "        x, gt = rawpy.imread(x_img_path), rawpy.imread(gt_img_path)\n",
    "        x = np.expand_dims(pack_raw(x), axis=0) * ratio\n",
    "        \n",
    "        gt = gt.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "        gt = np.expand_dims(np.float32(gt / 65535.0), axis=0)\n",
    "        sample = {low_exp_img: x, high_exp_img: gt}\n",
    "        if self.transforms != None:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample\n",
    "        return str(np.random.randint(100, 1000)) + '--' + str(idx) + ' -- ' + str(self.Xs[idx])\n",
    "        \n",
    "lki = None\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size) -> None:\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.h, self.w = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.h, self.w = output_size\n",
    "    def __call__(self, sample):\n",
    "        global lki\n",
    "        lli, hli = sample[low_exp_img], sample[high_exp_img]\n",
    "        lki = lli\n",
    "        # print(lli.shape, hli.shape)\n",
    "        or_h, or_w = lli.shape[1:3]\n",
    "        # print('height width: ', or_h, or_w)\n",
    "        start_height = torch.randint(0, or_h-self.h, size=(1,))\n",
    "        start_width = torch.randint(0, or_w-self.w, size=(1,))\n",
    "        # print(start_height, start_width)\n",
    "        lli = lli[:, start_height:start_height+self.h, start_width:start_width+self.w]\n",
    "        hli = hli[:, start_height*2:start_height*2+self.h*2, start_width*2:start_width*2+self.w*2]\n",
    "        # print(lli.shape, hli.shape)\n",
    "        return {low_exp_img: lli, high_exp_img:hli}\n",
    "\n",
    "class RandomFlip(object):\n",
    "    def __init__(self, probabilty=.3):\n",
    "        self.probabilty = probabilty\n",
    "    def __call__(self, sample):\n",
    "        lli, hli = sample[low_exp_img], sample[high_exp_img]\n",
    "        hor_prob = torch.rand(1)[0]\n",
    "        ver_prob = torch.rand(1)[0]\n",
    "        if hor_prob > self.probabilty:\n",
    "            lli = np.flip(lli, axis=1)\n",
    "            hli = np.flip(hli, axis=1)\n",
    "        if ver_prob > self.probabilty:\n",
    "            lli = np.flip(lli, axis=2)\n",
    "            hli = np.flip(hli, axis=2)\n",
    "        return {low_exp_img: lli, high_exp_img:hli}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        lli, hli = sample[low_exp_img].copy(), sample[high_exp_img].copy()\n",
    "        # print('coming_shape: ', lli.shape, hli.shape)\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        lli = lli.transpose((0, 3, 1, 2))\n",
    "        hli = hli.transpose((0, 3, 1, 2))\n",
    "        lli = lli[0]\n",
    "        hhi = hli[0]\n",
    "        return {low_exp_img: torch.from_numpy(lli),\n",
    "                high_exp_img: torch.from_numpy(hli)}\n",
    "\n",
    "\n",
    "# im_size = (512, 512)\n",
    "# composer = transforms.Compose([\n",
    "#     RandomCrop(im_size),\n",
    "#     # torchvision.transforms.RandomEqualize(.2),\n",
    "#     RandomFlip(.3),\n",
    "#     # torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     # torchvision.transforms.RandomVerticalFlip(),\n",
    "#     ToTensor(),\n",
    "# ])\n",
    "\n",
    "# im_size = (512, 512)\n",
    "# ds = Dataset('./Sony/Sony_train_list.txt', res=im_size, transforms=composer)\n",
    "# for _, batch in enumerate(ds):\n",
    "#     sample = batch\n",
    "#     break\n",
    "# print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    print(batch.shape)\n",
    "    \n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "workers = 0\n",
    "im_size = (512, 512)\n",
    "\n",
    "composer = transforms.Compose([\n",
    "    RandomCrop(im_size),\n",
    "    # torchvision.transforms.RandomEqualize(.2),\n",
    "    RandomFlip(.3),\n",
    "    # torchvision.transforms.RandomHorizontalFlip(),\n",
    "    # torchvision.transforms.RandomVerticalFlip(),\n",
    "    ToTensor(),\n",
    "])\n",
    "ds = Dataset('./Sony/Sony_train_list.txt', res=im_size, transforms=composer)\n",
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class OriginalModel(torch.nn.Module):\n",
    "    def __init__(self, block_size, in_channel=4, kernel_size=3, dialation=1) -> None:\n",
    "        super().__init__()\n",
    "        self.block_size = block_size \n",
    "        \n",
    "        self.activation = torch.nn.LeakyReLU(.02, True)\n",
    "        self.max_pool = torch.nn.MaxPool2d(2, 2, 0, ceil_mode=True)\n",
    "        \n",
    "        self.convF1 = torch.nn.Conv2d(in_channel, 32, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convF2 = torch.nn.Conv2d(32, 32, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.convF3 = torch.nn.Conv2d(32, 64, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convF4 = torch.nn.Conv2d(64, 64, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.convF5 = torch.nn.Conv2d(64, 128, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convF6 = torch.nn.Conv2d(128, 128, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.convF7 = torch.nn.Conv2d(128, 256, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convF8 = torch.nn.Conv2d(256, 256, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.convF9 = torch.nn.Conv2d(256, 512, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convF10 = torch.nn.Conv2d(512, 512, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.conv_upB10 = torch.nn.ConvTranspose2d(512, 256, 2, 2, bias=False)\n",
    "        self.convB10 = torch.nn.Conv2d(512, 256, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convB9 = torch.nn.Conv2d(256, 256, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.conv_upB8 = torch.nn.ConvTranspose2d(256, 128, 2, 2, bias=False)\n",
    "        self.convB8 = torch.nn.Conv2d(256, 128, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convB7 = torch.nn.Conv2d(128, 128, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.conv_upB6 = torch.nn.ConvTranspose2d(128, 64, 2, 2, bias=False)\n",
    "        self.convB6 = torch.nn.Conv2d(128, 64, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convB5 = torch.nn.Conv2d(64, 64, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.conv_upB4 = torch.nn.ConvTranspose2d(64, 32, 2, 2, bias=False)\n",
    "        self.convB4 = torch.nn.Conv2d(64, 32, kernel_size, 1, 1, dialation, bias=True)\n",
    "        self.convB3 = torch.nn.Conv2d(32, 32, kernel_size, 1, 1, dialation, bias=True)\n",
    "        \n",
    "        self.convB = torch.nn.Conv2d(32, 3 * self.block_size * self.block_size, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def init_weights(self, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, torch.nn.ConvTranspose2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "    def forward(self, x):\n",
    "        x = self.convF1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convF2(x)\n",
    "        x = self.activation(x)\n",
    "        up2 = x\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.convF3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convF4(x)\n",
    "        x = self.activation(x)\n",
    "        up4 = x\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.convF5(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convF6(x)\n",
    "        x = self.activation(x)\n",
    "        up6 = x\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.convF7(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convF8(x)\n",
    "        x = self.activation(x)\n",
    "        up8 = x\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.convF9(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convF10(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv_upB10(x)\n",
    "        x = torch.cat((x[:, :, :up8.size(2), :up8.size(3)], up8), 1)\n",
    "        x = self.convB10(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convB9(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv_upB8(x)\n",
    "        x = torch.cat((x[:, :, :up6.size(2), :up6.size(3)], up6), 1)\n",
    "        x = self.convB8(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convB7(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv_upB6(x)\n",
    "        x = torch.cat((x[:, :, :up4.size(2), :up4.size(3)], up4), 1)\n",
    "        x = self.convB6(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convB5(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        \n",
    "        x = self.conv_upB4(x)\n",
    "        x = torch.cat((x[:, :, :up2.size(2), :up2.size(3)], up2), 1)\n",
    "        x = self.convB4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.convB3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.convB(x)\n",
    "        x = torch.nn.PixelShuffle(2)(x)\n",
    "        return x\n",
    "        \n",
    "    def pixel_shuffle(x, upscale_factor, depth_first=False):\n",
    "        pass\n",
    "model = OriginalModel(2, 4, 3).to(device=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([4, 3, 1024, 1024])) that is different to the input size (torch.Size([4, 1, 3, 1024, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "d:\\Python\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([1, 3, 1024, 1024])) that is different to the input size (torch.Size([1, 1, 3, 1024, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][32/467]\tloss::3613104.000000\trunning_loss::19.375000\n",
      "[2/50][65/467]\tloss::4018040.750000\trunning_loss::18.671875\n",
      "[3/50][98/467]\tloss::17288642.000000\trunning_loss::53.812500\n",
      "[4/50][131/467]\tloss::15243571.000000\trunning_loss::83.617188\n",
      "[5/50][164/467]\tloss::184733.187500\trunning_loss::126.988281\n",
      "[6/50][197/467]\tloss::27397.541016\trunning_loss::110.121094\n",
      "[7/50][230/467]\tloss::488240.125000\trunning_loss::202.833008\n",
      "[8/50][263/467]\tloss::72211.601562\trunning_loss::237.835938\n",
      "[9/50][296/467]\tloss::11608.757812\trunning_loss::233.623535\n",
      "[10/50][329/467]\tloss::114759.156250\trunning_loss::241.376709\n",
      "[11/50][362/467]\tloss::2668.846191\trunning_loss::348.927490\n",
      "[12/50][395/467]\tloss::2342.109131\trunning_loss::336.605835\n",
      "[13/50][428/467]\tloss::2828.647217\trunning_loss::288.806030\n",
      "[14/50][461/467]\tloss::742392.312500\trunning_loss::198.055298\n",
      "[16/50][27/467]\tloss::177769.750000\trunning_loss::2.937500\n",
      "[17/50][60/467]\tloss::251716.171875\trunning_loss::5.432617\n",
      "[18/50][93/467]\tloss::9956.503906\trunning_loss::37.465332\n",
      "[19/50][126/467]\tloss::21859.396484\trunning_loss::48.182007\n",
      "[20/50][159/467]\tloss::10220.958008\trunning_loss::75.358521\n",
      "[21/50][192/467]\tloss::7633.166504\trunning_loss::95.484772\n",
      "[22/50][225/467]\tloss::7116.280762\trunning_loss::48.804977\n",
      "[23/50][258/467]\tloss::5772.308105\trunning_loss::39.178436\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41272/1340830882.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mllis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlow_exp_img\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhigh_exp_img\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mllis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# print(hlis.shape, outputs.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optim = torch.optim.NAdam(model.parameters(),lr=.003)\n",
    "mse = torch.nn.MSELoss()\n",
    "l1 = torch.nn.L1Loss()\n",
    "epochs = 50\n",
    "print_every = 500 # steps\n",
    "iters = 0\n",
    "running_loss = 0.0\n",
    "for epoch in range(epochs): \n",
    "    for idx, batched in enumerate(dl, 0):\n",
    "        # model.zero_grad()\n",
    "        optim.zero_grad()\n",
    "        llis, hlis = batched[low_exp_img], batched[high_exp_img]\n",
    "        outputs = model(llis.to(device=cuda))\n",
    "        outputs = outputs.to('cpu')\n",
    "        # print(hlis.shape, outputs.shape)\n",
    "        \n",
    "        err = l1(hlis, outputs)\n",
    "        err.backward()\n",
    "        optim.step()\n",
    "        running_loss += err.item()\n",
    "        iters+=1\n",
    "        if (iters % print_every == 0) or ((epoch == epochs-1) and (idx == len(dl)-1)):\n",
    "            print('[%d/%d][%d/%d]\\tloss::%f\\trunning_loss::%f' % (epoch, epochs, idx, len(dl), err, running_loss / (idx+1)))\n",
    "    running_loss = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "torch.save(model.state_dict(), './lsid__'+str(time.time())[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0986,  0.1367,  0.0976,  ...,  0.1384,  0.0972,  0.1366],\n",
       "          [ 0.0895, -0.1348,  0.0930,  ..., -0.1365,  0.0916, -0.1350],\n",
       "          [ 0.0977,  0.1377,  0.0962,  ...,  0.1367,  0.0988,  0.1370],\n",
       "          ...,\n",
       "          [ 0.0902, -0.1346,  0.0916,  ..., -0.1365,  0.0915, -0.1353],\n",
       "          [ 0.0939,  0.1359,  0.0949,  ...,  0.1352,  0.1003,  0.1364],\n",
       "          [ 0.0892, -0.1375,  0.0879,  ..., -0.1381,  0.0912, -0.1351]],\n",
       "\n",
       "         [[-0.1606, -0.1566, -0.1600,  ..., -0.1598, -0.1591, -0.1572],\n",
       "          [-0.1528,  0.0398, -0.1522,  ...,  0.0350, -0.1534,  0.0360],\n",
       "          [-0.1615, -0.1575, -0.1610,  ..., -0.1607, -0.1593, -0.1576],\n",
       "          ...,\n",
       "          [-0.1520,  0.0409, -0.1532,  ...,  0.0366, -0.1544,  0.0376],\n",
       "          [-0.1619, -0.1581, -0.1620,  ..., -0.1593, -0.1621, -0.1555],\n",
       "          [-0.1541,  0.0406, -0.1563,  ...,  0.0405, -0.1549,  0.0413]],\n",
       "\n",
       "         [[ 0.1573, -0.0244,  0.1572,  ..., -0.0239,  0.1585, -0.0254],\n",
       "          [ 0.0619, -0.1601,  0.0620,  ..., -0.1608,  0.0618, -0.1582],\n",
       "          [ 0.1575, -0.0257,  0.1596,  ..., -0.0252,  0.1624, -0.0260],\n",
       "          ...,\n",
       "          [ 0.0589, -0.1600,  0.0619,  ..., -0.1614,  0.0593, -0.1575],\n",
       "          [ 0.1597, -0.0243,  0.1600,  ..., -0.0238,  0.1631, -0.0251],\n",
       "          [ 0.0584, -0.1572,  0.0605,  ..., -0.1585,  0.0575, -0.1573]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0987,  0.1375,  0.0975,  ...,  0.1376,  0.0972,  0.1362],\n",
       "          [ 0.0892, -0.1349,  0.0935,  ..., -0.1366,  0.0922, -0.1350],\n",
       "          [ 0.0978,  0.1381,  0.0959,  ...,  0.1368,  0.0976,  0.1367],\n",
       "          ...,\n",
       "          [ 0.0909, -0.1359,  0.0929,  ..., -0.1367,  0.0913, -0.1354],\n",
       "          [ 0.0945,  0.1367,  0.0946,  ...,  0.1356,  0.0997,  0.1364],\n",
       "          [ 0.0902, -0.1374,  0.0898,  ..., -0.1384,  0.0903, -0.1359]],\n",
       "\n",
       "         [[-0.1605, -0.1568, -0.1607,  ..., -0.1583, -0.1597, -0.1568],\n",
       "          [-0.1536,  0.0392, -0.1513,  ...,  0.0365, -0.1528,  0.0367],\n",
       "          [-0.1608, -0.1582, -0.1609,  ..., -0.1598, -0.1609, -0.1566],\n",
       "          ...,\n",
       "          [-0.1533,  0.0387, -0.1538,  ...,  0.0375, -0.1540,  0.0379],\n",
       "          [-0.1612, -0.1580, -0.1626,  ..., -0.1590, -0.1619, -0.1554],\n",
       "          [-0.1542,  0.0394, -0.1566,  ...,  0.0400, -0.1553,  0.0409]],\n",
       "\n",
       "         [[ 0.1576, -0.0249,  0.1574,  ..., -0.0238,  0.1581, -0.0251],\n",
       "          [ 0.0617, -0.1599,  0.0617,  ..., -0.1597,  0.0619, -0.1583],\n",
       "          [ 0.1573, -0.0256,  0.1584,  ..., -0.0250,  0.1620, -0.0260],\n",
       "          ...,\n",
       "          [ 0.0598, -0.1608,  0.0608,  ..., -0.1600,  0.0591, -0.1580],\n",
       "          [ 0.1592, -0.0236,  0.1611,  ..., -0.0241,  0.1629, -0.0248],\n",
       "          [ 0.0589, -0.1586,  0.0585,  ..., -0.1577,  0.0577, -0.1570]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0984,  0.1374,  0.0973,  ...,  0.1376,  0.0973,  0.1361],\n",
       "          [ 0.0892, -0.1350,  0.0930,  ..., -0.1362,  0.0923, -0.1347],\n",
       "          [ 0.0977,  0.1380,  0.0958,  ...,  0.1367,  0.0976,  0.1368],\n",
       "          ...,\n",
       "          [ 0.0907, -0.1360,  0.0928,  ..., -0.1369,  0.0921, -0.1352],\n",
       "          [ 0.0944,  0.1365,  0.0946,  ...,  0.1352,  0.1001,  0.1364],\n",
       "          [ 0.0902, -0.1374,  0.0900,  ..., -0.1372,  0.0910, -0.1351]],\n",
       "\n",
       "         [[-0.1604, -0.1569, -0.1603,  ..., -0.1583, -0.1602, -0.1566],\n",
       "          [-0.1537,  0.0391, -0.1520,  ...,  0.0369, -0.1526,  0.0371],\n",
       "          [-0.1609, -0.1582, -0.1610,  ..., -0.1598, -0.1615, -0.1565],\n",
       "          ...,\n",
       "          [-0.1534,  0.0385, -0.1538,  ...,  0.0365, -0.1545,  0.0369],\n",
       "          [-0.1613, -0.1581, -0.1625,  ..., -0.1588, -0.1618, -0.1554],\n",
       "          [-0.1541,  0.0396, -0.1565,  ...,  0.0405, -0.1549,  0.0412]],\n",
       "\n",
       "         [[ 0.1579, -0.0249,  0.1576,  ..., -0.0239,  0.1580, -0.0253],\n",
       "          [ 0.0617, -0.1597,  0.0623,  ..., -0.1601,  0.0621, -0.1584],\n",
       "          [ 0.1575, -0.0259,  0.1592,  ..., -0.0256,  0.1618, -0.0262],\n",
       "          ...,\n",
       "          [ 0.0600, -0.1608,  0.0609,  ..., -0.1609,  0.0590, -0.1574],\n",
       "          [ 0.1591, -0.0237,  0.1610,  ..., -0.0241,  0.1631, -0.0252],\n",
       "          [ 0.0589, -0.1585,  0.0587,  ..., -0.1585,  0.0580, -0.1575]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0985,  0.1375,  0.0977,  ...,  0.1400,  0.0993,  0.1379],\n",
       "          [ 0.0892, -0.1349,  0.0936,  ..., -0.1383,  0.0903, -0.1363],\n",
       "          [ 0.0977,  0.1380,  0.0960,  ...,  0.1382,  0.0996,  0.1366],\n",
       "          ...,\n",
       "          [ 0.0904, -0.1360,  0.0926,  ..., -0.1353,  0.0916, -0.1357],\n",
       "          [ 0.0946,  0.1365,  0.0946,  ...,  0.1341,  0.1006,  0.1358],\n",
       "          [ 0.0896, -0.1375,  0.0892,  ..., -0.1357,  0.0930, -0.1337]],\n",
       "\n",
       "         [[-0.1605, -0.1567, -0.1602,  ..., -0.1621, -0.1579, -0.1588],\n",
       "          [-0.1537,  0.0392, -0.1512,  ...,  0.0307, -0.1562,  0.0333],\n",
       "          [-0.1611, -0.1583, -0.1607,  ..., -0.1608, -0.1604, -0.1599],\n",
       "          ...,\n",
       "          [-0.1531,  0.0393, -0.1536,  ...,  0.0326, -0.1579,  0.0354],\n",
       "          [-0.1615, -0.1578, -0.1623,  ..., -0.1590, -0.1623, -0.1558],\n",
       "          [-0.1543,  0.0399, -0.1565,  ...,  0.0406, -0.1547,  0.0420]],\n",
       "\n",
       "         [[ 0.1578, -0.0247,  0.1572,  ..., -0.0239,  0.1596, -0.0261],\n",
       "          [ 0.0618, -0.1596,  0.0617,  ..., -0.1595,  0.0620, -0.1591],\n",
       "          [ 0.1574, -0.0255,  0.1590,  ..., -0.0266,  0.1643, -0.0279],\n",
       "          ...,\n",
       "          [ 0.0596, -0.1610,  0.0606,  ..., -0.1622,  0.0579, -0.1569],\n",
       "          [ 0.1595, -0.0239,  0.1605,  ..., -0.0262,  0.1649, -0.0261],\n",
       "          [ 0.0586, -0.1581,  0.0592,  ..., -0.1603,  0.0569, -0.1579]]]],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs = model(llis.to(cuda))\n",
    "outputs.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somer\\AppData\\Local\\Temp/ipykernel_7024/4093345015.py:19: DeprecationWarning: This function is deprecated. Please call randint(0, 2 + 1) instead\n",
      "  in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageSizes(raw_height=2848, raw_width=4288, height=2848, width=4256, top_margin=0, left_margin=0, iheight=2848, iwidth=4256, pixel_aspect=1.0, flip=0)\n",
      "ImageSizes(raw_height=2848, raw_width=4288, height=2848, width=4256, top_margin=0, left_margin=0, iheight=2848, iwidth=4256, pixel_aspect=1.0, flip=0)\n",
      "(2848, 4256, 3)\n",
      "(1, 2848, 4256, 3)\n",
      "(1, 512, 512, 4) (1, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "input_dir = './Sony/Sony/short/'\n",
    "gt_dir = './Sony/Sony/long/'\n",
    "train_fns = glob.glob(gt_dir + '0*.ARW')\n",
    "train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n",
    "\n",
    "ps = 512  # patch size for training\n",
    "save_freq = 500\n",
    "\n",
    "gt_images = [None] * 6000\n",
    "input_images = {}\n",
    "input_images['300'] = [None] * len(train_ids)\n",
    "input_images['250'] = [None] * len(train_ids)\n",
    "input_images['100'] = [None] * len(train_ids)\n",
    "for ind in np.random.permutation(len(train_ids)):\n",
    "        # get the path from image id\n",
    "        train_id = train_ids[ind]\n",
    "        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n",
    "        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n",
    "        in_fn = os.path.basename(in_path)\n",
    "\n",
    "        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id)\n",
    "        gt_path = gt_files[0]\n",
    "        gt_fn = os.path.basename(gt_path)\n",
    "        in_exposure = float(in_fn[9:-5])\n",
    "        gt_exposure = float(gt_fn[9:-5])\n",
    "        ratio = min(gt_exposure / in_exposure, 300)\n",
    "\n",
    "        if input_images[str(ratio)[0:3]][ind] is None:\n",
    "            raw = rawpy.imread(in_path)\n",
    "            print(raw.sizes)\n",
    "            input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
    "\n",
    "            gt_raw = rawpy.imread(gt_path)\n",
    "            print(gt_raw.sizes)\n",
    "            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "            print(im.shape)\n",
    "            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
    "            print(gt_images[ind].shape)\n",
    "\n",
    "        # crop\n",
    "        H = input_images[str(ratio)[0:3]][ind].shape[1]\n",
    "        W = input_images[str(ratio)[0:3]][ind].shape[2]\n",
    "\n",
    "        xx = np.random.randint(0, W - ps)\n",
    "        yy = np.random.randint(0, H - ps)\n",
    "        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n",
    "        # gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
    "        gt_patch = gt_images[ind][:, yy:yy + ps, xx:xx + ps, :]\n",
    "        print(input_patch.shape, gt_patch.shape)\n",
    "\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random flip\n",
    "            input_patch = np.flip(input_patch, axis=1)\n",
    "            gt_patch = np.flip(gt_patch, axis=1)\n",
    "        if np.random.randint(2, size=1)[0] == 1:\n",
    "            input_patch = np.flip(input_patch, axis=2)\n",
    "            gt_patch = np.flip(gt_patch, axis=2)\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n",
    "            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
    "            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n",
    "\n",
    "        # input_patch = np.minimum(input_patch, 1.0)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('a', np.concatenate((input_patch[0, :, :, 1:], np.minimum(input_patch, 1.0)[0, :, :, 1:]), axis=1))\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('b', gt_patch[0])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Sony/short/00001_00_0.04s.ARW -- ./Sony/long/00001_00_10s.ARW ISO200 F8\n",
      "\n",
      "00001\n"
     ]
    }
   ],
   "source": [
    "with open('./Sony/Sony_train_list.txt') as f:\n",
    "    lines = f.readlines()b\n",
    "    for line in lines:\n",
    "        idx = line.find(' ')\n",
    "        print(line[:idx], '--', line[idx+1:])\n",
    "        img_idx = line[:idx].split('/')[-1][:5]\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2848, 4256, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(np.float32(im / 65535.0), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1424, 2128, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.expand_dims(pack_raw(raw), axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6056)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_path, main_dir='Sony/', res=(512,512), tp='bayer', transforms=None):\n",
    "        self.main_dir = main_dir\n",
    "        self.Xs = []\n",
    "        self.X_images = {}\n",
    "        self.GT_images = {}\n",
    "        with open(text_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                idx = line.find(' ')\n",
    "                # self.Xs[line[:idx]] = line[idx+1:]\n",
    "                self.Xs.append((line[:idx], line[idx+1:]))\n",
    "        self.transforms = transforms\n",
    "        self.ids = np.arange(len(self.Xs))\n",
    "    def __len__(self):\n",
    "        return len(self.Xs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # if idx == 0: \n",
    "        #     self.ids = np.random.permutation(self.ids)\n",
    "        # return torch.rand(1024), torch.rand(1024)\n",
    "        x_img_info, gt_img_info = self.Xs[self.ids[idx]]\n",
    "        x_img_path = os.path.join(self.main_dir, x_img_info)\n",
    "        gt_img_path = gt_img_info.split(' ')[0]\n",
    "        gt_img_path = os.path.join(self.main_dir, gt_img_path)\n",
    "        x, gt = None, None \n",
    "        if x_img_path in self.X_images.keys():\n",
    "            x = self.X_images[x_img_path]\n",
    "        else:\n",
    "            base_x, base_gt = x_img_path.split('/')[-1], gt_img_path.split('/')[-1]\n",
    "            in_exposure = float(base_x[9:-5])\n",
    "            gt_exposure = float(base_gt[9:-5])\n",
    "            ratio = min(gt_exposure / in_exposure, 300)\n",
    "            x = rawpy.imread(x_img_path)\n",
    "            x = np.expand_dims(pack_raw(x), axis=0) * ratio\n",
    "            self.X_images[x_img_path] = x\n",
    "        if gt_img_path in self.GT_images.keys():\n",
    "            gt = self.GT_images[gt_img_path]\n",
    "        else: \n",
    "            gt = rawpy.imread(gt_img_path)\n",
    "            gt = gt.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "            gt = np.expand_dims(np.float32(gt / 65535.0), axis=0)\n",
    "            self.GT_images[gt_img_path] = gt\n",
    "            \n",
    "        # print(x_img_path, gt_img_path)\n",
    "        \n",
    "        # x, gt = rawpy.imread(x_img_path), rawpy.imread(gt_img_path)\n",
    "        # gt = np.expand_dims(np.float32(gt / 65535.0), axis=0)\n",
    "        sample = {low_exp_img: x, high_exp_img: gt}\n",
    "        if self.transforms != None:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset2('./Sony/Sony_train_list.txt', res=im_size, transforms=composer)\n",
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([4, 3, 1024, 1024])) that is different to the input size (torch.Size([4, 1, 3, 1024, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41272/3110876416.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mllis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlow_exp_img\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhigh_exp_img\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mllis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# print(hlis.shape, outputs.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optim = torch.optim.NAdam(model.parameters(),lr=.003)\n",
    "mse = torch.nn.MSELoss()\n",
    "l1 = torch.nn.L1Loss()\n",
    "epochs = 50\n",
    "print_every = 250 # steps\n",
    "iters = 0\n",
    "running_loss = 0.0\n",
    "for epoch in range(epochs): \n",
    "    for idx, batched in enumerate(dl, 0):\n",
    "        # model.zero_grad()\n",
    "        optim.zero_grad()\n",
    "        llis, hlis = batched[low_exp_img], batched[high_exp_img]\n",
    "        outputs = model(llis.to(device=cuda))\n",
    "        outputs = outputs.to('cpu')\n",
    "        # print(hlis.shape, outputs.shape)\n",
    "        \n",
    "        err = l1(hlis, outputs)\n",
    "        err.backward()\n",
    "        optim.step()\n",
    "        running_loss += err.item()\n",
    "        iters+=1\n",
    "        if (iters % print_every == 0) or ((epoch == epochs-1) or (idx == len(dl)-1)):\n",
    "            print('[%d/%d][%d/%d]\\tloss::%f\\trunning_loss::%f' % (epoch, epochs, idx, len(dl), err, running_loss / (idx+1)))\n",
    "    running_loss = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckck\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 46.2 MiB for an array with shape (2848, 4256) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41272/1565154543.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0miters\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miters\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41272/2078020377.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_exposure\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0min_exposure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrawpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_img_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_img_path\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgt_img_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGT_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41272/2629186390.py\u001b[0m in \u001b[0;36mpack_raw\u001b[1;34m(raw)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# pack Bayer image to 4 channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_image_visible\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m16383\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# subtract the black level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 46.2 MiB for an array with shape (2848, 4256) and data type float32"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): \n",
    "    for idx, batched in enumerate(dl, 0):\n",
    "        iters+=1\n",
    "        \n",
    "        if (iters % print_every == 0) or ((epoch == epochs-1) or (idx == len(dl)-1)):\n",
    "            print(\"ckck\")\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.NAdam(model.parameters(),lr=.003)\n",
    "sch = torch.optim.lr_scheduler.StepLR(\n",
    "        optim, step_size  = 10 , gamma = 0.5)\n",
    "lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, 'min', .5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_exp_img = 'lei'\n",
    "high_exp_img = 'hei'\n",
    "\n",
    "def pack_raw(raw):\n",
    "    # pack Bayer image to 4 channels\n",
    "    im = raw.raw_image_visible.astype(np.float32)\n",
    "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
    "\n",
    "    im = np.expand_dims(im, axis=2)\n",
    "    img_shape = im.shape\n",
    "    H = img_shape[0]\n",
    "    W = img_shape[1]\n",
    "\n",
    "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
    "                          im[0:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_path, main_dir='Sony/', res=(512,512), tp='bayer', transforms=None):\n",
    "        self.main_dir = main_dir\n",
    "        self.Xs = []\n",
    "        with open(text_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                idx = line.find(' ')\n",
    "                # self.Xs[line[:idx]] = line[idx+1:]\n",
    "                self.Xs.append((line[:idx], line[idx+1:]))\n",
    "        self.transforms = transforms\n",
    "        self.ids = np.arange(len(self.Xs))\n",
    "    def __len__(self):\n",
    "        return len(self.Xs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0: \n",
    "            self.ids = np.random.permutation(self.ids)\n",
    "        # sample = {low_exp_img: np.random.rand(1, 1024, 1024,4), high_exp_img: np.random.rand(1, 2048, 2048 ,3)}\n",
    "        # if self.transforms != None:\n",
    "        #     sample = self.transforms(sample)\n",
    "        # return sample\n",
    "    \n",
    "        # return torch.rand(1024), torch.rand(1024)\n",
    "        x_img_info, gt_img_info = self.Xs[self.ids[idx]]\n",
    "        x_img_path = os.path.join(self.main_dir, x_img_info)\n",
    "        gt_img_path = gt_img_info.split(' ')[0]\n",
    "        gt_img_path = os.path.join(self.main_dir, gt_img_path)\n",
    "        # print(x_img_path, gt_img_path)\n",
    "        base_x, base_gt = x_img_path.split('/')[-1], gt_img_path.split('/')[-1]\n",
    "        in_exposure = float(base_x[9:-5])\n",
    "        gt_exposure = float(base_gt[9:-5])\n",
    "        ratio = min(gt_exposure / in_exposure, 300)\n",
    "        x, gt = rawpy.imread(x_img_path), rawpy.imread(gt_img_path)\n",
    "        x = np.expand_dims(pack_raw(x), axis=0) * ratio\n",
    "        \n",
    "        gt = gt.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "        gt = np.expand_dims(np.float32(gt / 65535.0), axis=0)\n",
    "        print(x.shape, gt.shape)\n",
    "        sample = {low_exp_img: x, high_exp_img: gt}\n",
    "        if self.transforms != None:\n",
    "            sample = self.transforms(sample)\n",
    "        print(x.dtype)\n",
    "        return sample\n",
    "        return str(np.random.randint(100, 1000)) + '--' + str(idx) + ' -- ' + str(self.Xs[idx])\n",
    "        \n",
    "lki = None\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size) -> None:\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.h, self.w = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.h, self.w = output_size\n",
    "    def __call__(self, sample):\n",
    "        global lki\n",
    "        lli, hli = sample[low_exp_img], sample[high_exp_img]\n",
    "        lki = lli\n",
    "        # print(lli.shape, hli.shape)\n",
    "        or_h, or_w = lli.shape[1:3]\n",
    "        # print('height width: ', or_h, or_w)\n",
    "        start_height = torch.randint(0, or_h-self.h, size=(1,))\n",
    "        start_width = torch.randint(0, or_w-self.w, size=(1,))\n",
    "        # print(start_height, start_width)\n",
    "        lli = lli[:, start_height:start_height+self.h, start_width:start_width+self.w]\n",
    "        hli = hli[:, start_height*2:start_height*2+self.h*2, start_width*2:start_width*2+self.w*2]\n",
    "        # print(lli.shape, hli.shape)\n",
    "        return {low_exp_img: lli, high_exp_img:hli}\n",
    "\n",
    "class RandomFlip(object):\n",
    "    def __init__(self, probabilty=.3):\n",
    "        self.probabilty = probabilty\n",
    "    def __call__(self, sample):\n",
    "        lli, hli = sample[low_exp_img], sample[high_exp_img]\n",
    "        hor_prob = torch.rand(1)[0]\n",
    "        ver_prob = torch.rand(1)[0]\n",
    "        if hor_prob > self.probabilty:\n",
    "            lli = np.flip(lli, axis=1)\n",
    "            hli = np.flip(hli, axis=1)\n",
    "        if ver_prob > self.probabilty:\n",
    "            lli = np.flip(lli, axis=2)\n",
    "            hli = np.flip(hli, axis=2)\n",
    "        return {low_exp_img: lli, high_exp_img:hli}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        lli, hli = sample[low_exp_img].copy(), sample[high_exp_img].copy()\n",
    "        # print('coming_shape: ', lli.shape, hli.shape)\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        lli = lli.transpose((0, 3, 1, 2))\n",
    "        hli = hli.transpose((0, 3, 1, 2))\n",
    "        lli = lli[0]\n",
    "        hhi = hli[0]\n",
    "        return {low_exp_img: torch.from_numpy(lli),\n",
    "                high_exp_img: torch.from_numpy(hli)}\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "workers = 0\n",
    "im_size = (512, 512)\n",
    "\n",
    "composer = transforms.Compose([\n",
    "    RandomCrop(im_size),\n",
    "    # torchvision.transforms.RandomEqualize(.2),\n",
    "    RandomFlip(.3),\n",
    "    # torchvision.transforms.RandomHorizontalFlip(),\n",
    "    # torchvision.transforms.RandomVerticalFlip(),\n",
    "    ToTensor(),\n",
    "])\n",
    "ds = Dataset('./Sony/Sony_train_list.txt', res=im_size, transforms=composer)\n",
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1424, 2128, 4) (1, 2848, 4256, 3)\n",
      "float32\n",
      "(1, 1424, 2128, 4) (1, 2848, 4256, 3)\n",
      "float32\n",
      "(1, 1424, 2128, 4) (1, 2848, 4256, 3)\n",
      "float32\n",
      "(1, 1424, 2128, 4) (1, 2848, 4256, 3)\n",
      "float32\n",
      "torch.Size([4, 4, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([4, 3, 1024, 1024])) that is different to the input size (torch.Size([4, 1, 3, 1024, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32728/4240942365.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[%d/%d][%d/%d]\\tloss::%f\\trunning_loss::%f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'metrics'"
     ]
    }
   ],
   "source": [
    "model.to(cuda)\n",
    "optim = torch.optim.NAdam(model.parameters(),lr=.003)\n",
    "sch = torch.optim.lr_scheduler.StepLR(optim, step_size  = 10 , gamma = 0.5)\n",
    "lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, 'min', .5, patience=10)\n",
    "l1 = torch.nn.L1Loss()\n",
    "epochs = 50\n",
    "print_every = 500 # steps\n",
    "iters = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    for idx, batched in enumerate(dl, 0):\n",
    "        # model.zero_grad()\n",
    "        optim.zero_grad()\n",
    "        llis, hlis = batched[low_exp_img], batched[high_exp_img]\n",
    "        print(llis.shape)\n",
    "        outputs = model(llis.to(device=cuda))\n",
    "        outputs = outputs.to('cpu')\n",
    "        # print(hlis.shape, outputs.shape)\n",
    "        \n",
    "        err = l1(hlis, outputs)\n",
    "        err.backward()\n",
    "        optim.step()\n",
    "        running_loss += err.item()\n",
    "        iters+=1\n",
    "        if (iters % print_every == 0) or ((epoch == epochs-1) and (idx == len(dl)-1)):\n",
    "            print('[%d/%d][%d/%d]\\tloss::%f\\trunning_loss::%f' % (epoch, epochs, idx, len(dl), err, running_loss / (idx+1)))\n",
    "        break\n",
    "    lr.step()\n",
    "    running_loss = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
